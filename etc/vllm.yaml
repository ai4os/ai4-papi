---
# In non-quantized models, we have to specify "--dtype float16" because the default
# bfloat16 is only supported in GPUs with compute capability +8.0 (NVIDIA T4 has 7.5).

# Models are sorted in rough performance order

models:
  Qwen/Qwen2.5-7B-Instruct-AWQ:
    name: 'Qwen2.5-7B-Instruct-AWQ'
    description: 'A 7B parameter instruction-tuned model from the Qwen 2.5 series, optimized for dialogue and general-purpose tasks.'
    family: 'Qwen'
    license: 'Apache 2.0'
    context: '128K'
    needs_HF_token: False
    args: ['--quantization', 'awq']

  Qwen/Qwen3-4B-AWQ:
    name: 'Qwen3-4B-AWQ'
    description: 'A 4B parameter instruction-tuned model from the Qwen 3 series, optimized for reasoning, instruction-following, agent capabilities, and multilingual support.'
    family: 'Qwen'
    license: 'Apache 2.0'
    context: '32.7K'
    needs_HF_token: False
    args: ['--dtype', 'float16', '--max-model-len', '8192']

  Qwen/Qwen3-8B-AWQ:
    name: 'Qwen3-8B-AWQ'
    description: 'A 8B parameter instruction-tuned model from the Qwen 3 series, optimized for reasoning, instruction-following, agent capabilities, and multilingual support.'
    family: 'Qwen'
    license: 'Apache 2.0'
    context: '32.7K'
    needs_HF_token: False
    args: ['--dtype', 'float16', '--max-model-len', '8192']

  Qwen/Qwen3-14B-AWQ:
    name: 'Qwen3-14B-AWQ'
    description: 'A 14B parameter instruction-tuned model from the Qwen 3 series, optimized for reasoning, instruction-following, agent capabilities, and multilingual support.'
    family: 'Qwen'
    license: 'Apache 2.0'
    context: '32.7K'
    needs_HF_token: False
    args: ['--dtype', 'float16', '--max-model-len', '8192']

  Qwen/Qwen3-0.6B:
    name: 'Qwen3-0.6B'
    description: 'A 0.6 parameter instruction-tuned model from the Qwen 3 series, optimized for reasoning, instruction-following, agent capabilities, and multilingual support.'
    family: 'Qwen'
    license: 'Apache 2.0'
    context: '32.7K'
    needs_HF_token: False
    args: ['--dtype', 'float16', '--max-model-len', '8192']

  Qwen/Qwen3-1.7B:
    name: 'Qwen3-1.7B'
    description: 'A 1.7 parameter instruction-tuned model from the Qwen 3 series, optimized for reasoning, instruction-following, agent capabilities, and multilingual support.'
    family: 'Qwen'
    license: 'Apache 2.0'
    context: '32.7K'
    needs_HF_token: False
    args: ['--dtype', 'float16', '--max-model-len', '8192']

  Qwen/Qwen3-4B:
    name: 'Qwen3-4B'
    description: 'A 4 parameter instruction-tuned model from the Qwen 3 series, optimized for reasoning, instruction-following, agent capabilities, and multilingual support.'
    family: 'Qwen'
    license: 'Apache 2.0'
    context: '32.7K'
    needs_HF_token: False
    args: ['--dtype', 'float16', '--max-model-len', '8192']

  Qwen/Qwen2.5-Coder-7B-Instruct-AWQ:
    name: 'Qwen2.5-Coder-7B-Instruct-AWQ'
    description: 'A 7B parameter model specialized in code generation and understanding, instruction-tuned for programming-related tasks.'
    family: 'Qwen'
    license: 'Apache 2.0'
    context: '131K'
    needs_HF_token: False
    args: ['--quantization', 'awq']

  Qwen/Qwen2.5-Math-1.5B-Instruct:
    name: 'Qwen2.5-Math-1.5B-Instruct'
    description: 'A 1.5B parameter model focused on mathematical reasoning and problem-solving, optimized for math-related queries.'
    family: 'Qwen'
    license: 'Apache 2.0'
    context: '32.7K'
    needs_HF_token: False
    args: ['--dtype', 'float16']

  deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B:
    name: 'DeepSeek-R1-Distill-Qwen-1.5B'
    description: 'A distilled version of DeepSeek-R1, based on Qwen architecture, optimized for efficient reasoning and inference tasks.'
    family: 'deepseek-ai'
    license: 'Apache 2.0'
    context: '131K'
    needs_HF_token: False
    args:
      [
        '--dtype',
        'float16',
        '--enable-reasoning',
        '--reasoning-parser',
        'deepseek_r1',
      ]

  meta-llama/Llama-3.2-3B:
    name: 'Llama-3.2-3B'
    description: 'A 3B parameter model from Metaâ€™s Llama 3.2 series, designed for general NLP tasks with improved efficiency.'
    family: 'meta-llama'
    license: 'Custom Meta AI License'
    context: '128K'
    needs_HF_token: True
    args: ['--dtype', 'float16']

  meta-llama/Llama-3.2-3B-Instruct:
    name: 'Llama-3.2-3B-Instruct'
    description: 'An instruction-tuned variant of Llama-3.2-3B, optimized for chatbot-like interactions and structured responses.'
    family: 'meta-llama'
    license: 'Custom Meta AI License'
    context: '131K'
    needs_HF_token: True
    args: ['--dtype', 'float16']

  # TODO: Add this small vision model in the future. It does not work with the current Transformers library.
  # ERROR 02-12 04:48:33 engine.py:389] ValueError: The checkpoint you are trying to load has model type `qwen2_5_vl` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.

  # Qwen/Qwen2.5-VL-3B-Instruct:
  #   needs_HF_token: False
  #   args: [
  #     "--dtype", "float16",
  #   ]
