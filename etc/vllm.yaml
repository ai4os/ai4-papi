---
# In non-quantized models, we have to specify "--dtype float16" because the default
# bfloat16 is only supported in GPUs with compute capability +8.0 (NVIDIA T4 has 7.5).

models:
  deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B:
    needs_HF_token: False
    args: ['--dtype', 'float16']

  # Add this small vision model in the future. It does not work with the current Transformers library.
  # ERROR 02-12 04:48:33 engine.py:389] ValueError: The checkpoint you are trying to load has model type `qwen2_5_vl` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.

  # Qwen/Qwen2.5-VL-3B-Instruct:
  #   needs_HF_token: False
  #   args: [
  #     "--dtype", "float16",
  #   ]

  Qwen/Qwen2.5-7B-Instruct-AWQ:
    needs_HF_token: False
    args: ['--quantization', 'awq']

  Qwen/Qwen2.5-Coder-7B-Instruct-AWQ:
    needs_HF_token: False
    args: ['--quantization', 'awq']

  Qwen/Qwen2.5-Math-1.5B-Instruct:
    needs_HF_token: False
    args: ['--dtype', 'float16']

  meta-llama/Llama-3.2-3B:
    needs_HF_token: True
    args: ['--dtype', 'float16']

  meta-llama/Llama-3.2-3B-Instruct:
    needs_HF_token: True
    args: ['--dtype', 'float16']
