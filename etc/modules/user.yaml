---
# User customizable configuration to make a deployment in Nomad.
# Additional non-customizable values (eg. ports) are hardcoded in `job.nomad`.

# All conf parameters follow the same structure:
# varname:
#  name: name of the parameter to be displayed to end user (mandatory)
#  value: (default) value of the parameter (mandatory)
#  options: restricted set of values that the parameter can take (optional)
#  description: some comments on the parameter to be displayed to the end user (optional)

# CPU/RAM limits are based on the current have GPU flavours we (IFCA) have deployed:
# * g13-gpuib-8-86:
#     - 8 GPUs Nvidia Tesla T4
#     - 86 VCPUs --> ~9.6 cores / gpu (reserving 10% for the node)
#     - 351 GB RAM --> ~40 GB / gpu (reserving 10% for the node)
# * g12-gpuib-2-64:
#     - 2 GPUs Nvidia Tesla V100
#     - 64 VCPUs --> ~28 cores / gpu (reserving 10% for the node)
#     - 127 GB RAM --> ~57 GB / gpu (reserving 10% for the node)

general:
  title:
    name: Deployment title
    value: ''
    description: Provide short title for this deployment (less than 45 characters). Useful when you have lots of different active deployments.

  desc:
    name: Deployment description
    value: ''
    description: Provide some additional extended information about this deployment.

  docker_image:
    name: Docker image
    value: 'deephdc/deep-oc-image-classification-tf'
    description: Docker image to be used. For example `deephdc/deep-oc-image-classification-tf`.

  docker_tag:
    name: Docker tag
    value: 'latest'
    description: Docker tag to use. Tags are module dependent. You should choose the appropriate tag for your selected hardware (eg. use a `gpu`-like tag if you plan to run on GPUs).
    options: ['latest']

  service:
    name: Service to run
    value: 'deepaas'
    description: When selecting DEEPaaS you won't be able to use JupyterLab, and vice versa. If you want to have access to both services at the same time your best option is to deploy with JupyterLab, then open a terminal window and run DEEPaaS yourself typing `deep-start --deepaas`. In some container, you will also be able to deploy VS Code.
    options: ['deepaas', 'jupyter']

  jupyter_password:
    name: IDE password
    value: ''
    description: Select a password for your IDE (JupyterLab or VS Code). It should have at least 9 characters.

hardware:
  cpu_num:
    name: Number of CPUs
    value: 4
    range: [1, 10]

  gpu_num:
    name: Number of GPUs
    value: 0
    range: [0, 1]

  gpu_type:
    name: GPU model
    value: ''
    options: ['']
    description: Fill this field only if you have a *hard* requirement for a given kind of GPU model.

  ram:
    name: RAM memory (in MB)
    value: 8000
    range: [2000, 40000]

  disk:
    name: Disk memory (in MB)
    value: 10000
    range: [1000, 50000]

storage:
  rclone_conf:
    name: RCLONE configuration
    value: '/srv/.rclone/rclone.conf'
    description: rclone.conf location

  rclone_url:
    name: Storage URL
    value: ''
    description: Remote storage link to be accessed via rclone (webdav). For example, in Nextcloud `https://share.services.ai4os.eu/remote.php/dav/files/<USER-ID>`

  rclone_vendor:
    name: RCLONE vendor
    value: 'nextcloud'
    options: ['nextcloud']
    description: RCLONE vendor (webdav)

  rclone_user:
    name: RCLONE user
    value: ''
    description: rclone user to access a webdav remote storage

  rclone_password:
    name: RCLONE user password
    value: ''

  datasets:
    name: Info of the datasets you want to download
    value: []
    description: Each element in the list should be a dict containing "doi" and "force_pull" keys. It requires the definition of all RCLONE variables.
